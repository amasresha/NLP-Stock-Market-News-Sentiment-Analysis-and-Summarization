{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c99d46dd",
   "metadata": {
    "id": "EvCcfwuSU-fz"
   },
   "source": [
    "## **Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf6c8f",
   "metadata": {
    "id": "6QR_RHvIVHT2"
   },
   "source": [
    "### Business Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c36024d",
   "metadata": {
    "id": "pl3dmH-EnJGl"
   },
   "source": [
    "The prices of the stocks of companies listed under a global exchange are influenced by a variety of factors, with the company's financial performance, innovations and collaborations, and market sentiment being factors that play a significant role. News and media reports can rapidly affect investor perceptions and, consequently, stock prices in the highly competitive financial industry. With the sheer volume of news and opinions from a wide variety of sources, investors and financial analysts often struggle to stay updated and accurately interpret its impact on the market. As a result, investment firms need sophisticated tools to analyze market sentiment and integrate this information into their investment strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b84665",
   "metadata": {
    "id": "Vn6bbxSwVKl3"
   },
   "source": [
    "### Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adf029",
   "metadata": {
    "id": "jCIswL3zobj6"
   },
   "source": [
    "With an ever-rising number of news articles and opinions, an investment startup aims to leverage artificial intelligence to address the challenge of interpreting stock-related news and its impact on stock prices. They have collected historical daily news for a specific company listed under NASDAQ, along with data on its daily stock price and trade volumes.\n",
    "\n",
    "As a member of the Data Science and AI team in the startup, you have been tasked with analyzing the data, developing an AI-driven sentiment analysis system that will automatically process and analyze news articles to gauge market sentiment, and summarizing the news at a weekly level to enhance the accuracy of their stock price predictions and optimize investment strategies. This will empower their financial analysts with actionable insights, leading to more informed investment decisions and improved client outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724a3a0",
   "metadata": {
    "id": "ZJOtDHVSF5hu"
   },
   "source": [
    "### Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f0eac",
   "metadata": {
    "id": "ZlkjI8V5F9RK"
   },
   "source": [
    "* `Date` : The date the news was released\n",
    "* `News` : The content of news articles that could potentially affect the company's stock price\n",
    "* `Open` : The stock price (in \\$) at the beginning of the day\n",
    "* `High` : The highest stock price (in \\$) reached during the day\n",
    "* `Low` :  The lowest stock price (in \\$) reached during the day\n",
    "* `Close` : The adjusted stock price (in \\$) at the end of the day\n",
    "* `Volume` : The number of shares traded during the day\n",
    "* `Label` : The sentiment polarity of the news content\n",
    "    * 1: positive\n",
    "    * 0: neutral\n",
    "    * -1: negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a74f8",
   "metadata": {
    "id": "VrFQHcW5mYgv"
   },
   "source": [
    "## **Installing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1993d",
   "metadata": {
    "id": "A-E2-iaumpo8"
   },
   "outputs": [],
   "source": [
    "# Install the sentence-transformers library, which provides easy access to a variety of pre-trained transformer models.\n",
    "# These models are optimized for creating high-quality sentence embeddings.\n",
    "!pip install -U sentence-transformers -q\n",
    "\n",
    "# Install the gensim library, a well-known toolkit for natural language processing that includes word embedding models such as Word2Vec.\n",
    "!pip install -U gensim -q\n",
    "\n",
    "# Install the transformers library, maintained by Hugging Face, for a wide range of pre-trained transformer-based models.\n",
    "!pip install -U transformers -q\n",
    "\n",
    "# Install tqdm, a Python package that provides progress bars to help track the installation process or model training progress.\n",
    "!pip install -U tqdm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c93518",
   "metadata": {
    "id": "qNJIc0nqIclc"
   },
   "source": [
    "## Import the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7a110",
   "metadata": {
    "id": "1q7tOlgEHqSJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1573533",
   "metadata": {
    "id": "wQ46zPgumfjF"
   },
   "source": [
    "## **Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa14017",
   "metadata": {
    "id": "S04c7UZQLMup"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "try:\n",
    "  # Force remount Google Drive to refresh the connection\n",
    "  drive.mount('/content/drive', force_remount=True)  # Added force_remount=True\n",
    "  print(\"Drive mounted successfully!\")\n",
    "except ValueError as e:\n",
    "  print(f\"Mount failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edda051",
   "metadata": {
    "id": "ehzMVFw0Lbwl"
   },
   "outputs": [],
   "source": [
    "path='/content/drive/My Drive/AIMLTRAINING/collabdata/NLP/stock_news.csv'\n",
    "df=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7aaa21",
   "metadata": {
    "id": "EvFNfrvGWthn"
   },
   "source": [
    "## **Data Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0813bbd",
   "metadata": {
    "id": "bbH_NB6oyQKT"
   },
   "outputs": [],
   "source": [
    "# Display basic information about the DataFrame\n",
    "print(\"Data Information:\")\n",
    "print(\"=\"*30)\n",
    "print(df.info())\n",
    "\n",
    "# Show summary statistics for numerical columns\n",
    "print(\"\\nData Description:\")\n",
    "print(\"=\"*30)\n",
    "print(df.describe().T)\n",
    "\n",
    "# Display shape of the dataset (number of rows and columns)\n",
    "print(\"\\nDataset Shape:\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n",
    "# Preview the first few rows of the dataset\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "print(\"=\"*30)\n",
    "print(df.head())\n",
    "\n",
    "# Check for any missing values in each column\n",
    "print(\"\\nMissing Values per Column:\")\n",
    "print(\"=\"*30)\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Count the number of duplicate rows in the dataset\n",
    "print(\"\\nNumber of Duplicate Rows:\")\n",
    "print(\"=\"*30)\n",
    "print(df.duplicated().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b024de5a",
   "metadata": {
    "id": "cQv-dWvYNGEX"
   },
   "source": [
    "### Data Insights:\n",
    "\n",
    "- **Data Information:**\n",
    "  - The dataset contains 349 entries and 8 columns.\n",
    "  - Columns include stock prices, volume, date, news, and sentiment label.\n",
    "\n",
    "- **Data Description:**\n",
    "  - Stock prices (Open, High, Low, Close) range from $36.25 to $68.81.\n",
    "  - Average volume traded: ~128M, with a range up to 244M.\n",
    "  - Sentiment label is mostly neutral with a mean of -0.05.\n",
    "\n",
    "- **Dataset Shape:**\n",
    "  - 349 rows, 8 columns (manageable for analysis).\n",
    "\n",
    "- **First 5 Rows:**\n",
    "  - Stock prices remain the same for multiple news articles on the same date.\n",
    "\n",
    "- **Missing Values:**\n",
    "  - No missing values in any column.\n",
    "\n",
    "- **Duplicate Rows:**\n",
    "  - No duplicate rows, ensuring data uniqueness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d031f78c",
   "metadata": {
    "id": "hGHBK8-QeKOB"
   },
   "source": [
    "## **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d21a1",
   "metadata": {
    "id": "Q0UlMQnyegl7"
   },
   "source": [
    "### Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37de89a",
   "metadata": {
    "id": "9GVt_AAbe29X"
   },
   "source": [
    "* Distribution of individual variables\n",
    "* Compute and check the distribution of the length of news content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fd302d",
   "metadata": {
    "id": "R5r0PXbrN3Z8"
   },
   "source": [
    "### **Date** Univariate Analysis\n",
    "- Since the `Date` column is of object type, let's convert it to `datetime` for easier manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b7450f",
   "metadata": {
    "id": "b62m8r4jNeGm"
   },
   "outputs": [],
   "source": [
    "#convert Date column into 'date' type\n",
    "df['Date']=pd.to_datetime(df['Date'])\n",
    "\n",
    "\n",
    "#Check the data type of \"Date\"\n",
    "print(df['Date'].dtype)\n",
    "\n",
    "#print sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc7fdb7",
   "metadata": {
    "id": "mSMaZ_a-Q6sY"
   },
   "outputs": [],
   "source": [
    "# Univariate analysis of the Date column\n",
    "# Print basic information and key insights for the 'Date' column\n",
    "print('Number of Unique dates: ', len(df['Date'].unique()))\n",
    "print('Start and end date: ', df['Date'].min(), df['Date'].max())\n",
    "print('Range of the date: ', df['Date'].max() - df['Date'].min())\n",
    "print('Days with more than one record:', df['Date'].duplicated().sum())\n",
    "\n",
    "# Print the top 5 days with the most records\n",
    "print(\"Top 5 Days with the Most Records:\")\n",
    "print(df['Date'].value_counts().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3054b551",
   "metadata": {
    "id": "1TqYbURNUikQ"
   },
   "outputs": [],
   "source": [
    "# Group by Date and count the number of occurrences for each date\n",
    "date_counts = df['Date'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Plotting a bar chart to visualize the top 10 days with the most records\n",
    "plt.figure(figsize=(10, 6))\n",
    "date_counts.head(10).plot(kind='bar', color='orange')\n",
    "plt.title(\"Top 10 Days with the Most Records\", fontsize=14)\n",
    "plt.xlabel(\"Date\", fontsize=12)\n",
    "plt.ylabel(\"Number of Records\", fontsize=12)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092fbbd8",
   "metadata": {
    "id": "nFujmPXLWGn3"
   },
   "source": [
    "### Date Column Insights:\n",
    "\n",
    "1. The dataset spans **118 days** (from January 2 to April 30, 2019) with **71 unique dates**, indicating non-uniform data collection over the period.\n",
    "\n",
    "2. **January 3rd, 2019** has the highest number of records (28), suggesting significant events or noteworthy occurrences on that day.\n",
    "\n",
    "3. A total of **278 records** correspond to **multiple entries per day**, pointing to frequent updates or recurring events being logged on certain days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252c5a50",
   "metadata": {
    "id": "57tWY_GxwhRA"
   },
   "source": [
    "### **News** univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9740ffdd",
   "metadata": {
    "id": "jarthL_iwytM"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pprint\n",
    "df_news = pd.DataFrame()\n",
    "# Calculate the length of each news article and create a new column 'news_length'\n",
    "df_news['news_length'] = df['News'].apply(len)\n",
    "\n",
    "# Show basic descriptive statistics for the 'news_length' column and print them in a readable format\n",
    "print(\"Descriptive Statistics for 'news_length':\")\n",
    "print(df_news['news_length'].describe().to_string())\n",
    "\n",
    "# Download the list of stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the set of English stopwords to exclude common words from word count analysis\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Split news text into individual words, remove stop words, and count word frequencies\n",
    "word_counts = Counter(\n",
    "    word.lower() for text in df['News'] for word in text.split() if word.lower() not in stop\n",
    ")\n",
    "\n",
    "# Display the 10 most common words in the news articles with pretty print\n",
    "print(\"\\nTop 10 Most Common Words in News Articles:\")\n",
    "pprint.pprint(word_counts.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb150c36",
   "metadata": {
    "id": "_4Lr1NRVzwap"
   },
   "source": [
    "### `News` insights:\n",
    "\n",
    "1. **Article Length Consistency**: The average length of news articles is around 311 words, with most articles falling between 290 and 336 words. This suggests a fairly consistent length, likely suitable for a concise news format.\n",
    "\n",
    "2. **Focus on Apple and Related Topics**: The word frequency shows \"apple\" as the most common term, appearing 152 times, with related words like \"revenue\" and \"trade\" also frequent. This indicates that Apple’s financial performance and market-related topics are central themes in the dataset.\n",
    "\n",
    "3. **Emphasis on Market and Economic Conditions**: Other frequent terms, such as \"stock,\" \"U.S.,\" and \"company,\" point to a broader focus on stock market trends and economic conditions that may impact Apple's business or the tech sector in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f91a1",
   "metadata": {
    "id": "IXPsoex-1hEn"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# Download stopwords if needed\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define stop words\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# # Summaries for the Stock Price and Volume Columns\n",
    "# print(\"Descriptive Statistics for Stock Prices and Volume:\")\n",
    "# print(df[['Open', 'High', 'Low', 'Close', 'Volume']].describe().to_string())\n",
    "\n",
    "# Histograms and Summary for each Stock Price Column\n",
    "stock_price_columns = ['Open', 'High', 'Low', 'Close']\n",
    "for col in stock_price_columns:\n",
    "    # print(f\"\\nData Summary for {col}:\")\n",
    "    # print(df[col].describe().to_string())  # Print data summary\n",
    "\n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = df[col].hist(bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title(f\"Distribution of {col} Price\")\n",
    "    plt.xlabel(f\"{col} Price ($)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "\n",
    "    # Add frequency count on top of each bar\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', fontsize=10, color='black', xytext=(0, 10),\n",
    "                    textcoords='offset points')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Volume Column Analysis\n",
    "# print(\"\\nData Summary for Volume:\")\n",
    "# print(df['Volume'].describe().to_string())  # Print data summary\n",
    "\n",
    "# Histogram for Volume\n",
    "plt.figure(figsize=(8, 5))\n",
    "ax = df['Volume'].hist(bins=20, color='salmon', edgecolor='black')\n",
    "plt.title(\"Distribution of Trade Volume\")\n",
    "plt.xlabel(\"Volume\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "# Add frequency count on top of each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=10, color='black', xytext=(0, 10),\n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Box Plots for Stock Price Columns to Identify Outliers\n",
    "for col in stock_price_columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    ax = df.boxplot(column=col)\n",
    "    plt.title(f\"{col} Price Box Plot\")\n",
    "    plt.ylabel(\"Price ($)\")\n",
    "\n",
    "    # Calculate the outliers using the IQR method\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Find outliers\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    # print(f\"\\nOutliers in {col}:\")\n",
    "    # print(outliers[[col]])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Sentiment Label Analysis\n",
    "# print(\"\\nSentiment Label Distribution:\")\n",
    "label_counts = df['Label'].value_counts()\n",
    "# print(label_counts)\n",
    "\n",
    "# Bar Chart for Sentiment Label Distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = label_counts.plot(kind='bar', color=['green', 'gray', 'red'])\n",
    "plt.title(\"Distribution of Sentiment Labels\")\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(ticks=[0, 1, 2], labels=['Positive', 'Neutral', 'Negative'], rotation=0)\n",
    "\n",
    "# Add counter at the top of each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', fontsize=10, color='black', xytext=(0, 10),\n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39cd8f5",
   "metadata": {
    "id": "uCR0Wr793Ub0"
   },
   "source": [
    "### **Insights for univariate analysis:**\n",
    "**Open Price**:\n",
    "   - The average opening price is **46.23**, with a standard deviation of **6.44**, indicating moderate volatility in the stock’s opening price over the period.\n",
    "   - There are **outliers** at **66.82** and **64.33**, suggesting occasional price spikes that could be of interest for further analysis, as these prices are significantly higher than the 75th percentile of **50.71**.\n",
    "\n",
    "**High Price**:\n",
    "   - The mean high price is **46.70**, and the **max price** is **67.06**, indicating that the stock occasionally peaks at higher values, possibly due to market events or high demand.\n",
    "   - Similar to the Open price, **outliers** are observed at **67.06** and **64.88**, signaling unusual market behavior during certain periods.\n",
    "\n",
    "**Low Price**:\n",
    "   - The low price has an average value of **45.75**, with prices typically fluctuating between **41.48** and **49.78**.\n",
    "   - Outliers are visible at **65.86** and **62.29**, which are significantly above the typical range for low prices, suggesting these periods could involve significant price drops or market disruptions.\n",
    "\n",
    "**Close Price**:\n",
    "   - The average closing price is **44.93**, slightly lower than the other price columns, indicating a potential downward trend or market correction toward the end of the trading session.\n",
    "   - Outliers are at **64.81** and **62.57**, with these values being much higher than the typical closing range of **49.11** and **40.25**, indicating rare spikes in the stock's closing value.\n",
    "\n",
    "**Volume:**\n",
    "1. The mean trading volume is **128.95 million** shares with a standard deviation of **43.17 million**, indicating substantial variation in the volume of trades over the period.\n",
    "2. The minimum volume observed is **45.45 million**, while the maximum volume peaks at **244.44 million**, signaling periods of both low and very high trading activity, which may be linked to key events affecting stock prices.\n",
    "\n",
    "**Sentiment Labels:**\n",
    "1. The sentiment distribution shows that **170 instances** (about **49%**) are labeled as **neutral** (Label 0), which is the most common sentiment, reflecting periods of market stability.\n",
    "2. The **negative sentiment** label (Label -1) appears in **99 instances**, showing a notable portion of periods associated with market declines or pessimism. Meanwhile, **positive sentiment** (Label 1) is less frequent, with **80 instances** (about **23%**), possibly reflecting periods of optimism or growth in the market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81440b4",
   "metadata": {
    "id": "hLE0s7OFKilB"
   },
   "source": [
    "### Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e413896",
   "metadata": {
    "id": "Yn_9wfzxL-r1"
   },
   "source": [
    "* Correlation\n",
    "* Sentiment Polarity vs Price\n",
    "* Date vs Price\n",
    "\n",
    "**Note**: The above points are listed to provide guidance on how to approach bivariate analysis. Analysis has to be done beyond the above listed points to get maximum scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8639d",
   "metadata": {
    "id": "UV0tyQSLYagW"
   },
   "source": [
    "### 1. Correlation Analysis\n",
    "- **Goal**: Examine correlations between numerical variables (e.g., Open, High, Low, Close, Volume) to understand how stock prices and trading volumes are interrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caf7d65",
   "metadata": {
    "id": "DmwIbKtoO3QL"
   },
   "outputs": [],
   "source": [
    "#correlation matrix between numerical columns\n",
    "corr_matrix=df[['Open','High','Close','Volume']].corr()\n",
    "# print(corr_matrix)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr_matrix,annot=True,cmap='coolwarm',center=0)\n",
    "plt.title(\"Correlation Matrix of Stock Prices and Volume\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d9491e",
   "metadata": {
    "id": "dJw7d2Twdu6D"
   },
   "source": [
    " **Correlations insights**:\n",
    "   - The `Open`, `High`, and `Close` prices show a very strong positive correlation with each other (close to 1.0), indicating they move together closely on a daily basis.\n",
    "   - `Volume` has a very low negative correlation with `Open`, `High`, and `Close` prices (around -0.07 to -0.08), suggesting a weak inverse relationship between trading volume and price movements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bd5e04",
   "metadata": {
    "id": "KWuY42N_Z60B"
   },
   "source": [
    "### 2. Sentiment Polarity vs. Price\n",
    "- **Goal**: Determine how sentiment (represented by Label: positive, neutral, or negative) impacts stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a26300",
   "metadata": {
    "id": "EoAR2E8SP06s"
   },
   "outputs": [],
   "source": [
    "# Group data by 'Label' and calculate mean prices for each sentiment category\n",
    "sentiment_price_summary = df.groupby('Label')[['Open', 'High', 'Low', 'Close']].mean()\n",
    "# print(sentiment_price_summary)\n",
    "\n",
    "# Visualize sentiment impact on closing price\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=sentiment_price_summary.reset_index(), x='Label', y='Close', palette=\"viridis\")\n",
    "plt.title(\"Average Closing Price by Sentiment Label\")\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Average Closing Price ($)\")\n",
    "plt.xticks(ticks=[0, 1, 2], labels=['Positive', 'Neutral', 'Negative'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa94d43",
   "metadata": {
    "id": "9EwYm43-eIBc"
   },
   "source": [
    "**Sentiment Vs Price insights**:\n",
    "   - For sentiment label -1 (negative sentiment), `Close` prices are generally lower, suggesting that negative sentiment might correspond to reduced stock prices.\n",
    "   - Neutral sentiment (label 0) and positive sentiment (label 1) show higher average `Close` prices compared to negative sentiment, potentially indicating that neutral or positive news has a stabilizing or slightly positive effect on stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d19fb1",
   "metadata": {
    "id": "ABdYoQldaAtI"
   },
   "source": [
    "### 3. Date vs. Price\n",
    "- **Goal**: Understand how stock prices fluctuate over time and identify any noticeable trends or seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb18f4",
   "metadata": {
    "id": "tpgJD9v6SKaA"
   },
   "outputs": [],
   "source": [
    "# Print the date range in the data to understand the time span covered\n",
    "print(\"Date range in the dataset:\")\n",
    "print(f\"Start date: {df['Date'].min()}\")\n",
    "print(f\"End date: {df['Date'].max()}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# # Print the first and last few rows of the DataFrame to verify date ordering\n",
    "# print(\"First few rows of the dataset:\")\n",
    "# print(df.head())\n",
    "# print(\"\\n\")\n",
    "# print(\"Last few rows of the dataset:\")\n",
    "# print(df.tail())\n",
    "# print(\"\\n\")\n",
    "\n",
    "# Calculate and print summary statistics for the 'Close' price\n",
    "print(\"Summary statistics for 'Close' price:\")\n",
    "print(df['Close'].describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Plot Close price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Date'], df['Close'], color='blue', label='Close Price')\n",
    "plt.title(\"Stock Closing Price Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Closing Price ($)\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72dd0a",
   "metadata": {
    "id": "ZBn-9YraebJW"
   },
   "source": [
    "**Date Range and Price Summary**:\n",
    "   - The dataset covers a four-month period from January 2, 2019, to April 30, 2019, which provides a short-term perspective on stock price and sentiment trends.\n",
    "   - The average `Close` price over this period is approximately \\$44.93, with a standard deviation of 6.4, showing moderate price volatility.\n",
    "   - The highest `Close` price is \\$64.80, indicating some significant upward price movements within this time frame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afdc806",
   "metadata": {
    "id": "ojPQMG-MaFfS"
   },
   "source": [
    "### 4. Sentiment Polarity vs. Volume\n",
    "- **Goal**: Assess if trading volume differs across sentiment labels, as higher volumes may indicate heightened investor reaction to certain types of news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90183d1",
   "metadata": {
    "id": "nEdbYhRvTCIa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print basic statistics for trading volume by sentiment polarity\n",
    "# print(\"Summary Statistics for Trading Volume by Sentiment Polarity:\\n\")\n",
    "volume_stats = df.groupby('Label')['Volume'].describe()\n",
    "# print(volume_stats)\n",
    "# print(\"\\nAverage Trading Volume for each Sentiment Label:\\n\")\n",
    "# print(volume_stats['mean'])\n",
    "\n",
    "# Set the style for the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Boxplot: Volume vs Sentiment Polarity\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x='Label', y='Volume', palette=\"coolwarm\")\n",
    "plt.title(\"Trading Volume by Sentiment Polarity\")\n",
    "plt.xlabel(\"Sentiment Polarity\")\n",
    "plt.ylabel(\"Trading Volume\")\n",
    "plt.xticks([0, 1, 2], [\"Negative\", \"Neutral\", \"Positive\"])\n",
    "plt.show()\n",
    "\n",
    "# Bar Plot: Average Volume by Sentiment Polarity\n",
    "volume_by_sentiment = df.groupby('Label')['Volume'].mean().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=volume_by_sentiment, x='Label', y='Volume', palette=\"coolwarm\")\n",
    "plt.title(\"Average Trading Volume by Sentiment Polarity\")\n",
    "plt.xlabel(\"Sentiment Polarity\")\n",
    "plt.ylabel(\"Average Trading Volume\")\n",
    "plt.xticks([0, 1, 2], [\"Negative\", \"Neutral\", \"Positive\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04006b9e",
   "metadata": {
    "id": "l24AcZ_wW6jZ"
   },
   "source": [
    "**Insights from Trading Volume by Sentiment Polarity**\n",
    "\n",
    "1. **Neutral Sentiment Shows the Highest Volume**: Neutral sentiment is associated with the highest average trading volume, suggesting significant market activity on days with ambiguous news.\n",
    "\n",
    "2. **Stable Volume for Positive Sentiment**: Positive news leads to stable trading volumes, indicating consistent investor confidence.\n",
    "\n",
    "3. **Negative Sentiment Drives High, Varied Volume**: Negative sentiment triggers high trading volumes but with more fluctuation, reflecting stronger emotional reactions to bad news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb87dc",
   "metadata": {
    "id": "7nZPSqB-mxN9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Additional Feature Calculations\n",
    "df['Price_Range'] = df['High'] - df['Low']  # Price volatility (High - Low)\n",
    "df['Short_Term_MA'] = df['Close'].rolling(window=10).mean()  # Short-term moving average\n",
    "df['Long_Term_MA'] = df['Close'].rolling(window=50).mean()  # Long-term moving average\n",
    "\n",
    "# Summary Statistics for Volume by Sentiment Polarity\n",
    "# print(\"\\nSummary Statistics for Trading Volume by Sentiment Polarity:\")\n",
    "# print(df.groupby('Label')['Volume'].describe())\n",
    "\n",
    "# Sentiment Polarity vs Trading Volume Boxplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Label', y='Volume', data=df)\n",
    "plt.title('Sentiment Polarity vs. Trading Volume')\n",
    "plt.show()\n",
    "\n",
    "# Sentiment Polarity vs Close Price Boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='Label', y='Close', data=df)\n",
    "plt.title('Sentiment Polarity vs. Close Price')\n",
    "plt.show()\n",
    "\n",
    "# Sentiment Polarity vs Price Volatility (Price Range)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Label', y='Price_Range', data=df)\n",
    "plt.title('Sentiment Polarity vs. Price Volatility (Price Range)')\n",
    "plt.show()\n",
    "\n",
    "# Sentiment Polarity and Moving Averages Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Date', y='Short_Term_MA', data=df, label='10-day MA')\n",
    "sns.lineplot(x='Date', y='Long_Term_MA', data=df, label='50-day MA')\n",
    "sns.scatterplot(x='Date', y='Label', data=df, label='Sentiment Polarity', color='blue', alpha=0.5)\n",
    "plt.title('Sentiment Polarity and Moving Averages')\n",
    "plt.show()\n",
    "\n",
    "# Rolling Correlation between Sentiment and Close Price\n",
    "df['Rolling_Correlation'] = df['Label'].rolling(window=30).corr(df['Close'])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df['Date'], df['Rolling_Correlation'], label='Rolling Correlation')\n",
    "plt.title('Rolling Correlation between Sentiment and Close Price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Correlation')\n",
    "plt.show()\n",
    "\n",
    "# Sentiment Polarity vs Trading Volume and Close Price Over Time\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Date', y='Volume', data=df, label='Volume')\n",
    "sns.scatterplot(x='Date', y='Label', data=df, color='blue', alpha=0.5, label='Sentiment Polarity')\n",
    "plt.title('Sentiment Polarity and Trading Volume Over Time')\n",
    "plt.show()\n",
    "\n",
    "# Additional Summary for Insights\n",
    "print(\"\\nAverage Trading Volume by Sentiment Label:\")\n",
    "print(df.groupby('Label')['Volume'].mean())\n",
    "\n",
    "print(\"\\nCorrelation between Sentiment and Close Price:\")\n",
    "print(df[['Label', 'Close']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf29e99",
   "metadata": {
    "id": "wU98k7i8BxQG"
   },
   "source": [
    "### Insights\n",
    "\n",
    "1. **Trading Volume by Sentiment**:\n",
    "   - Average trading volume is highest for neutral sentiment (`Label = 0`) at ~132M shares.\n",
    "   - Negative (`Label = -1`) and positive (`Label = 1`) sentiments show similar average trading volumes (~126M shares for negative, ~124M for positive).\n",
    "   - Neutral sentiment may generate higher trading activity, suggesting market stability or cautious investor behavior.\n",
    "\n",
    "2. **Volume Distribution by Sentiment**:\n",
    "   - The 25th and 75th percentiles for all sentiments fall within similar ranges, indicating relatively consistent volume across sentiments.\n",
    "   - Volume variability (std deviation) is slightly higher for neutral sentiment, suggesting more fluctuation when news is perceived as neutral.\n",
    "\n",
    "3. **Sentiment and Stock Price Correlation**:\n",
    "   - The correlation between sentiment (`Label`) and closing price (`Close`) is low (0.06), suggesting weak direct association.\n",
    "   - This may imply that news sentiment alone has limited immediate impact on daily stock prices, or that other factors (e.g., broader market conditions) dominate price movements.\n",
    "\n",
    "4. **Price Volatility and Sentiment** (from additional boxplots and statistics):\n",
    "   - Negative sentiment appears associated with higher price volatility (`Price_Range`), which could indicate investor reactions to negative news.\n",
    "   - Positive sentiment shows slightly lower volatility, aligning with potentially more stable or optimistic market conditions.\n",
    "\n",
    "5. **Additional Observations for Trading Strategies**:\n",
    "   - Higher trading volumes under neutral sentiment could imply a consolidation phase where prices fluctuate within a range.\n",
    "   - Low correlation may indicate the potential for lagged effects, suggesting further analysis with time-lagged sentiment features for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf1d01b",
   "metadata": {
    "id": "N8z4-vOBmwqv"
   },
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe805b4",
   "metadata": {
    "id": "I3wlydgR7I87"
   },
   "source": [
    "**Split the Target Variable and Predictors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac581da9",
   "metadata": {
    "id": "3nutDrxdzDee"
   },
   "outputs": [],
   "source": [
    "# Assuming 'df' is your DataFrame containing the data\n",
    "X = df.drop(columns=['Label'])  # Predictors\n",
    "y = df['Label']                 # Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ea75a",
   "metadata": {
    "id": "7eVro6Fp7WDH"
   },
   "source": [
    "**Split the Data into Train, Validation, and Test Sets**\n",
    "- Splitting the data helps to ensure the model is trained, validated, and tested on separate portions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4abe870",
   "metadata": {
    "id": "QUBEI1yF7aLq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and temporary (for validation and test) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90057702",
   "metadata": {
    "id": "AtxwoW7ncrAx"
   },
   "outputs": [],
   "source": [
    "#copy the original\n",
    "X_train_orig=X_train.copy()\n",
    "X_val_orig=X_val.copy()\n",
    "X_test_orig=X_test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1009289",
   "metadata": {
    "id": "kDJpciBR-TsR"
   },
   "source": [
    "**Preprocessing the Text Data**\n",
    "- Preprocessing involves tokenization, lowercasing, removing stopwords, punctuation, and stemming/lemmatization. This ensures that the model gets clean and meaningful inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67c1fc",
   "metadata": {
    "id": "Fxa4kU2e-QOx"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer and stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')).union(set(ENGLISH_STOP_WORDS))\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', str(text))  # Remove non-alphanumeric characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(text)\n",
    "\n",
    "# Apply preprocessing to news articles\n",
    "X_train['processed_news'] = X_train['News'].apply(preprocess_text)\n",
    "X_val['processed_news'] = X_val['News'].apply(preprocess_text)\n",
    "X_test['processed_news'] = X_test['News'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2385e9e",
   "metadata": {
    "id": "DNzU76hFYx1u"
   },
   "outputs": [],
   "source": [
    "#copy the preprossed data\n",
    "X_train_prepro=X_train.drop(columns=['News'])\n",
    "X_val_prepro= X_val.drop(columns=['News'])\n",
    "X_test_prepro= X_test.drop(columns=['News'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8361f1",
   "metadata": {
    "id": "0F5_cRZgcLxE"
   },
   "outputs": [],
   "source": [
    "X_train_prepro.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7b590d",
   "metadata": {
    "id": "P9kpkMt5vefa"
   },
   "outputs": [],
   "source": [
    "# Sample prints for Training Data\n",
    "print(\"\\nSample of Processed News from Training Set:\")\n",
    "print(X_train['processed_news'].head(3))\n",
    "\n",
    "# Sample prints for Validation Data\n",
    "print(\"\\nSample of Processed News from Validation Set:\")\n",
    "print(X_val['processed_news'].head(3))\n",
    "\n",
    "# Sample prints for Test Data\n",
    "print(\"\\nSample of Processed News from Test Set:\")\n",
    "print(X_test['processed_news'].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b841dce",
   "metadata": {
    "id": "0rYgR14ORf7b"
   },
   "source": [
    "## **Word Embeddings**\n",
    "\n",
    "- In this section, we will use three embedding techniques: Word2Vec, GloVe, and Sentence Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f156901",
   "metadata": {
    "id": "MDrMNpeFihKP"
   },
   "source": [
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24390091",
   "metadata": {
    "id": "3mzpwDpsh08g"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# split the text data into tokens (words) using NLTK's word_tokenize\n",
    "X_train['tokenized'] = X_train['processed_news'].apply(word_tokenize)\n",
    "X_val['tokenized'] = X_val['processed_news'].apply(word_tokenize)\n",
    "X_test['tokenized'] = X_test['processed_news'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c756e",
   "metadata": {
    "id": "UWHk9lceDFKS"
   },
   "outputs": [],
   "source": [
    "#sample of a tokenized news\n",
    "X_train['tokenized'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df589a8f",
   "metadata": {
    "id": "vJw-eDRa7zKu"
   },
   "source": [
    "**1. Word2Vec**\n",
    "- Word2Vec is a technique to map words into continuous vector representations based on their context in a corpus of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aeb550",
   "metadata": {
    "id": "H1_lN_o0zKlW"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 1. Trained a Word2Vec model on the tokenized training data to learn word embeddings\n",
    " # (vector representations of words).\n",
    "model_w2v = Word2Vec(sentences=X_train['tokenized'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get the word vectors for the training set (average of word vectors for each document)\n",
    "def get_word2vec_embeddings(tokens):\n",
    "    embeddings = [model_w2v.wv[word] for word in tokens if word in model_w2v.wv]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model_w2v.vector_size)\n",
    "\n",
    "# 2. Apply the model to transform each document into a vector by averaging the embeddings of the words in the document.\n",
    "X_train['w2v_embeddings'] = X_train['tokenized'].apply(get_word2vec_embeddings)\n",
    "X_val['w2v_embeddings'] = X_val['tokenized'].apply(get_word2vec_embeddings)\n",
    "X_test['w2v_embeddings'] = X_test['tokenized'].apply(get_word2vec_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaac9f2",
   "metadata": {
    "id": "w_K11ke5GorA"
   },
   "outputs": [],
   "source": [
    "# Sample of 10 Word2Vec embeddings (first 10 values) from the training set\n",
    "print(\"Sample of Word2Vec embeddings from the training data (first 10 values):\")\n",
    "print(X_train['w2v_embeddings'].head(1).apply(lambda x: x[:10]).to_list())\n",
    "\n",
    "# Sample of 10 Word2Vec embeddings (first 10 values) from the validation set\n",
    "print(\"\\nSample of Word2Vec embeddings from the validation data (first 10 values):\")\n",
    "print(X_val['w2v_embeddings'].head(1).apply(lambda x: x[:10]).to_list())\n",
    "\n",
    "# Sample of 10 Word2Vec embeddings (first 10 values) from the test set\n",
    "print(\"\\nSample of Word2Vec embeddings from the test data (first 10 values):\")\n",
    "print(X_test['w2v_embeddings'].head(1).apply(lambda x: x[:10]).to_list())\n",
    "\n",
    "# Print size of the embedding vector\n",
    "print(\"\\nSize of the Word2Vec embedding vector:\", len(X_train['w2v_embeddings'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745367d",
   "metadata": {
    "id": "5GI6N49yID7n"
   },
   "source": [
    "**2. GloVe**\n",
    "- We are going to use a pre-trained GloVe embedding file: glove.6B.100d.txt. This file contains word embeddings (vector representations) for a vocabulary of 6 billion tokens and 100 dimensions per word. It is pre-trained on large corpora like Wikipedia and Common Crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90ac3a",
   "metadata": {
    "id": "fhH6zUADI8Jz"
   },
   "outputs": [],
   "source": [
    "# 1. Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "# Path to the GloVe embeddings file\n",
    "glove_path = '/content/drive/My Drive/AIMLTRAINING/collabdata/NLP/glove.6B.100d.txt.word2vec'\n",
    "\n",
    "# Load GloVe embeddings from the provided path\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "\n",
    "# 2. Get GloVe embeddings for the training set (average of word vectors for each document)\n",
    "def get_glove_embeddings(tokens):\n",
    "    embeddings = [glove_embeddings[word] for word in tokens if word in glove_embeddings]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(100)  # 100 is the embedding size in this case\n",
    "\n",
    "# Apply GloVe embeddings to training, validation, and test sets\n",
    "X_train['glove_embeddings'] = X_train['tokenized'].apply(get_glove_embeddings)\n",
    "X_val['glove_embeddings'] = X_val['tokenized'].apply(get_glove_embeddings)\n",
    "X_test['glove_embeddings'] = X_test['tokenized'].apply(get_glove_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df644460",
   "metadata": {
    "id": "kSykDsUnJUBx"
   },
   "outputs": [],
   "source": [
    "# Sample of 10 GloVe embeddings (first 10 values) from the training set\n",
    "print(\"Sample of GloVe embeddings from the training data (first 10 values):\")\n",
    "print(X_train['glove_embeddings'].head(1).apply(lambda x: x[:10]).to_list())\n",
    "\n",
    "# Sample of 10 GloVe embeddings (first 10 values) from the validation set\n",
    "print(\"\\nSample of GloVe embeddings from the validation data (first 10 values):\")\n",
    "print(X_val['glove_embeddings'].head(1).apply(lambda x: x[:10]).to_list())\n",
    "\n",
    "# Sample of 10 GloVe embeddings (first 10 values) from the test set\n",
    "print(\"\\nSample of GloVe embeddings from the test data (first 10 values):\")\n",
    "print(X_test['glove_embeddings'].head(1).apply(lambda x: x[:10]).to_list())\n",
    "\n",
    "# Print size of the embedding vector\n",
    "print(\"\\nSize of the GloVe embedding vector:\", len(X_train['glove_embeddings'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6ae66",
   "metadata": {
    "id": "RU6IOnKkKvzS"
   },
   "source": [
    "**3. Sentence Transformer**\n",
    "- Sentence Transformers provide embeddings for entire sentences or documents, rather than individual words, making them ideal for tasks like document classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1e6860",
   "metadata": {
    "id": "i3kQksRuK9dC"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the model\n",
    "# The all-MiniLM-L6-v2 model is an all-round (all) model trained on a large and diverse dataset of over 1 billion training samples\n",
    "# and generates state-of-the-art sentence embeddings of 384 dimensions.\n",
    "model_st = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Get sentence embeddings for the training set\n",
    "X_train['st_embeddings'] = X_train['processed_news'].apply(lambda x: model_st.encode(x))\n",
    "X_val['st_embeddings'] = X_val['processed_news'].apply(lambda x: model_st.encode(x))\n",
    "X_test['st_embeddings'] = X_test['processed_news'].apply(lambda x: model_st.encode(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb41d92",
   "metadata": {
    "id": "VWTfrULkMBJb"
   },
   "outputs": [],
   "source": [
    "# Sample of 10 Sentence Transformer embeddings (first 10 values) from the training set\n",
    "print(\"Sample of Sentence Transformer embeddings from the training data (first 10 values):\")\n",
    "print(X_train['st_embeddings'].head(1).apply(lambda x: x[:10]).to_list())\n",
    "\n",
    "# Sample of 10 Sentence Transformer embeddings (first 10 values) from the validation set\n",
    "print(\"\\nSample of Sentence Transformer embeddings from the validation data (first 10 values):\")\n",
    "print(X_val['st_embeddings'].head(1).apply(lambda x: x[:10]).to_list())\n",
    "\n",
    "# Sample of 10 Sentence Transformer embeddings (first 10 values) from the test set\n",
    "print(\"\\nSample of Sentence Transformer embeddings from the test data (first 10 values):\")\n",
    "print(X_test['st_embeddings'].head(1).apply(lambda x: x[:10]).to_list())\n",
    "\n",
    "# Print size of the embedding vector\n",
    "print(\"\\nSize of the Sentence Transformer embedding vector:\", len(X_train['st_embeddings'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43a484",
   "metadata": {
    "id": "f4lwYN5bYmHp"
   },
   "source": [
    "## **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18271e6a",
   "metadata": {
    "id": "hvDMtcONmsmn"
   },
   "source": [
    "**Goal**\n",
    "- The goal of this project is to develop an AI-driven sentiment analysis system that analyzes stock-related news to gauge market sentiment, supporting improved stock price predictions and optimized investment strategies for more informed, effective decision-making.\n",
    "\n",
    "**Primary Metric Choice** - **F1-Score**\n",
    "\n",
    "**Why F1-Score?**\n",
    "- Given the three-class classification problem with an imbalanced dataset (170 neutral, 99 negative, and 80 positive labels), the F1-score is an ideal metric. It balances precision and recall, emphasizing the model's ability to correctly classify each sentiment label (positive, neutral, negative), while accounting for class imbalance.\n",
    "\n",
    "**Complementary Metric: Accuracy**\n",
    "\n",
    "**Why Accuracy?**\n",
    "- Accuracy offers a quick overview of correctness but can be misleading in imbalanced data, where the model might favor the majority class (neutral). As such, accuracy can serve as a supplementary measure to provide a broader perspective but isn’t suitable as the primary metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8cf7db",
   "metadata": {
    "id": "i63K05Ryon7O"
   },
   "source": [
    "**1. Basic Sentiment Analysis Using Word2Vec Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62778ebc",
   "metadata": {
    "id": "Hvn8PxDmz063"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Extract features (Word2Vec embeddings) from X_train, X_val, and X_test\n",
    "X_train_w2v = np.stack(X_train['w2v_embeddings'].values)\n",
    "X_val_w2v = np.stack(X_val['w2v_embeddings'].values)\n",
    "X_test_w2v = np.stack(X_test['w2v_embeddings'].values)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "model_w2v = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Predict on training, validation, and test sets\n",
    "y_train_pred = model_w2v.predict(X_train_w2v)\n",
    "y_val_pred = model_w2v.predict(X_val_w2v)\n",
    "y_test_pred = model_w2v.predict(X_test_w2v)\n",
    "\n",
    "# Print classification report for each set\n",
    "print(\"\\nTraining Set Classification Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f72bdf",
   "metadata": {
    "id": "EFFKrEFfrxMt"
   },
   "source": [
    "### Basic Word2Vec Model Performance Observations:\n",
    "\n",
    "1. **Overfitting**: Perfect training performance (F1, precision, recall = 1.0) suggests overfitting, indicating poor generalization to new data.\n",
    "\n",
    "2. **Poor Generalization**: Validation and test metrics are much lower (accuracy around 42-44%), showing the model struggles with unseen data.\n",
    "\n",
    "3. **Precision and Recall Imbalance**: The model has low precision and recall for minority classes, especially class `1`, indicating bias toward the majority class and potential issues with false positives and negatives.\n",
    "\n",
    "4. **Class Sensitivity**: Performance is better on the majority class (class `0`), indicating sensitivity to class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e3c33",
   "metadata": {
    "id": "nlXyiK9Ar_OS"
   },
   "source": [
    "**2. Basic Sentiment Analysis Using GloVe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7396a9",
   "metadata": {
    "id": "j9qv98shsRkp"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Extract GloVe embeddings for training, validation, and test sets\n",
    "X_train_glove = np.stack(X_train['glove_embeddings'].values)\n",
    "X_val_glove = np.stack(X_val['glove_embeddings'].values)\n",
    "X_test_glove = np.stack(X_test['glove_embeddings'].values)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "model_glove = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_glove.fit(X_train_glove, y_train)\n",
    "\n",
    "# Predict on training, validation, and test sets\n",
    "y_train_pred = model_glove.predict(X_train_glove)\n",
    "y_val_pred = model_glove.predict(X_val_glove)\n",
    "y_test_pred = model_glove.predict(X_test_glove)\n",
    "\n",
    "# Print classification report for each set\n",
    "print(\"\\nTraining Set Classification Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84e78f2",
   "metadata": {
    "id": "HkttFwX7sx1j"
   },
   "source": [
    "### Basic GloVe Model Performance Observations:\n",
    "\n",
    "1. **Overfitting**: Perfect scores on the training set (F1, precision, recall = 1.0) indicate strong overfitting, limiting generalization.\n",
    "\n",
    "2. **Poor Generalization**: Significant performance drop on validation and test sets, with low F1 scores (validation ~0.33, test ~0.23), showing the model struggles with unseen data.\n",
    "\n",
    "3. **High Recall, Low Precision**: While recall is higher for some classes, low precision on validation and test sets suggests many false positives, indicating imbalanced performance across classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9579f7c",
   "metadata": {
    "id": "eX1F4w1Ntbtn"
   },
   "source": [
    "**3. Basic Sentiment Analysis Using Sentence Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9fbfaa",
   "metadata": {
    "id": "gD6rCsHmt3Ki"
   },
   "outputs": [],
   "source": [
    "#  Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Extract the sentence embeddings for each set\n",
    "X_train_st = np.vstack(X_train['st_embeddings'].values)\n",
    "X_val_st = np.vstack(X_val['st_embeddings'].values)\n",
    "X_test_st = np.vstack(X_test['st_embeddings'].values)\n",
    "\n",
    "# Initialize a Random Forest classifier (or another model)\n",
    "model_st = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "model_st.fit(X_train_st, y_train)\n",
    "\n",
    "# Predict on training, validation, and test sets\n",
    "y_train_pred = model_st.predict(X_train_st)\n",
    "y_val_pred = model_st.predict(X_val_st)\n",
    "y_test_pred = model_st.predict(X_test_st)\n",
    "\n",
    "# Print classification report for each set\n",
    "print(\"\\nTraining Set Classification Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(\"-\" * 40)\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b5cdf",
   "metadata": {
    "id": "sSPL_e62xR6s"
   },
   "source": [
    "### Basic Sentence Transformer Model Performance Observation:\n",
    "\n",
    "1. **Overfitting**: The model achieves perfect scores on the training set (F1, accuracy = 1.0), which suggests overfitting and poor generalization to new data.\n",
    "\n",
    "2. **Poor Generalization**: Performance drops significantly on the validation (F1 ~0.49, accuracy ~0.54) and test sets (F1 ~0.24, accuracy ~0.38), indicating the model struggles to generalize effectively.\n",
    "\n",
    "3. **Class Imbalance Issues**: Precision for class 1 is zero, and recall is extremely low for certain classes, suggesting poor model performance on underrepresented classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb10d12",
   "metadata": {
    "id": "WX7ABRRNxWWP"
   },
   "source": [
    "### Hyperparameter Tuning\n",
    "**1. Tuned Sentiment Analysis Using Word2Vec Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab935e",
   "metadata": {
    "id": "yIyf3rKnxihS"
   },
   "outputs": [],
   "source": [
    "# **1. Sentiment Analysis Using Word2Vec Embeddings**\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Extract features (Word2Vec embeddings) from X_train, X_val, and X_test\n",
    "X_train_w2v = np.stack(X_train['w2v_embeddings'].values)\n",
    "X_val_w2v = np.stack(X_val['w2v_embeddings'].values)\n",
    "X_test_w2v = np.stack(X_test['w2v_embeddings'].values)\n",
    "\n",
    "# Set the best hyperparameters directly based on previous tuning\n",
    "best_params = {\n",
    "    'n_estimators': 50,\n",
    "    'max_depth': 10,\n",
    "    'min_samples_split': 2,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': 'log2'\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier with the best parameters\n",
    "model_w2v_tuned = RandomForestClassifier(random_state=42, **best_params)\n",
    "\n",
    "# Fit the model on the training data\n",
    "model_w2v_tuned.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Predict on training, validation, and test sets\n",
    "y_train_pred = model_w2v_tuned.predict(X_train_w2v)\n",
    "y_val_pred = model_w2v_tuned.predict(X_val_w2v)\n",
    "y_test_pred = model_w2v_tuned.predict(X_test_w2v)\n",
    "\n",
    "# Print classification report for each set\n",
    "print(\"\\nTraining Set Classification Report:\")\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea389b7e",
   "metadata": {
    "id": "zdB4JrLymQ8B"
   },
   "source": [
    "### Tuned Word2Vec Model Performance Insights:\n",
    "\n",
    "1. **Overfitting**: The model achieves perfect scores (F1 = 1.0) on the training set, indicating overfitting. This means the model fits too closely to the training data and likely struggles with generalization.\n",
    "\n",
    "2. **Improvement Over Untuned Model**: The tuned model shows slight improvements in validation (F1 = 0.395) and test performance (F1 = 0.341) compared to the untuned model, but overfitting still persists.\n",
    "\n",
    "3. **Class Imbalance**: The model struggles with class imbalance, particularly for class 1, where both precision and recall are low, especially on the test set.\n",
    "\n",
    "### Comparison with Untuned Word2Vec Model\n",
    "\n",
    "- **Validation and Test Performance**: Both the tuned and untuned models show poor generalization, with relatively low F1 scores and accuracy on the test set (F1 ~0.34 for tuned vs. ~0.33 for untuned).\n",
    "- **Slight Improvement**: The tuned model offers marginal improvement in precision and recall over the untuned model, but both still exhibit signs of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea5cf7",
   "metadata": {
    "id": "q14K3CqPywv4"
   },
   "source": [
    "**2. Tuned Sentiment Analysis Using GloVe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbf69d8",
   "metadata": {
    "id": "O0ta56FRn-dN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Extract GloVe embeddings for training, validation, and test sets\n",
    "X_train_glove = np.stack(X_train['glove_embeddings'].values)\n",
    "X_val_glove = np.stack(X_val['glove_embeddings'].values)\n",
    "X_test_glove = np.stack(X_test['glove_embeddings'].values)\n",
    "\n",
    "# Set the best hyperparameters directly based on previous tuning\n",
    "best_params_glove = {\n",
    "    'n_estimators': 50,\n",
    "    'learning_rate': 0.1,\n",
    "    'max_depth': 7,\n",
    "    'min_samples_split': 5,\n",
    "    'min_samples_leaf': 1\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting classifier with the best parameters\n",
    "model_glove_tuned = GradientBoostingClassifier(random_state=42, **best_params_glove)\n",
    "\n",
    "# Train the model on the training set\n",
    "model_glove_tuned.fit(X_train_glove, y_train)\n",
    "\n",
    "# Predict on training, validation, and test sets\n",
    "y_train_pred = model_glove_tuned.predict(X_train_glove)\n",
    "y_val_pred = model_glove_tuned.predict(X_val_glove)\n",
    "y_test_pred = model_glove_tuned.predict(X_test_glove)\n",
    "\n",
    "# Print classification report for each set\n",
    "print(\"\\nTraining Set Classification Report:\")\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de003103",
   "metadata": {
    "id": "p7a-Nw8QqEZA"
   },
   "source": [
    "### Tuned GloVe Model Insights:\n",
    "- **Training Set**: Perfect performance (F1 = 1.0, Accuracy = 1.0), indicating overfitting.\n",
    "- **Validation Set**: Moderate performance drop (F1 = 0.44, Accuracy = 0.46), still quite far from training set perfection, showing some overfitting.\n",
    "- **Test Set**: Further drop (F1 = 0.33, Accuracy = 0.36), with the model continuing to struggle with generalization to unseen data, especially with low recall for class -1 and class 1.\n",
    "\n",
    "### Comparison with Untuned/Basic GloVe Model:\n",
    "- The tuned model shows slight improvement over the untuned model, outperforming it on both the validation (F1 = 0.44 vs. 0.25) and test (F1 = 0.33 vs. 0.21) sets, but both still face significant challenges in generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaeee2b",
   "metadata": {
    "id": "3dNjG_iB7QTo"
   },
   "source": [
    "**3. Tuned Sentiment Analysis Using Sentence Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3959fcc0",
   "metadata": {
    "id": "dDuEUF0AsRBL"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Extract the sentence embeddings for each set\n",
    "X_train_st = np.vstack(X_train['st_embeddings'].values)\n",
    "X_val_st = np.vstack(X_val['st_embeddings'].values)\n",
    "X_test_st = np.vstack(X_test['st_embeddings'].values)\n",
    "\n",
    "# Combine train and validation sets for cross-validation\n",
    "X_combined = np.vstack((X_train_st, X_val_st))\n",
    "y_combined = np.hstack((y_train, y_val))\n",
    "\n",
    "# Set the best hyperparameters based on previous tuning results\n",
    "best_params_rf = {\n",
    "    'n_estimators': 50,\n",
    "    'max_depth': None,\n",
    "    'min_samples_split': 10,\n",
    "    'min_samples_leaf': 1,\n",
    "    'max_features': 'sqrt'\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest classifier with the best parameters\n",
    "model_st_tuned = RandomForestClassifier(random_state=42, **best_params_rf)\n",
    "\n",
    "# Train the model on the training set\n",
    "model_st_tuned.fit(X_train_st, y_train)\n",
    "\n",
    "# Predict on training, validation, and test sets\n",
    "y_train_pred = model_st_tuned.predict(X_train_st)\n",
    "y_val_pred = model_st_tuned.predict(X_val_st)\n",
    "y_test_pred = model_st_tuned.predict(X_test_st)\n",
    "\n",
    "# Print classification report for each set\n",
    "print(\"\\nTraining Set Classification Report:\")\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "\n",
    "print(\"\\nValidation Set Classification Report:\")\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nTest Set Classification Report:\")\n",
    "print(\"-\"*40)\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd520f9",
   "metadata": {
    "id": "jMU7V3niq9ue"
   },
   "source": [
    "### Insights on the Untuned Sentence Transformer Model:\n",
    "- **Training Set**: Perfect performance (F1, Accuracy, Precision, Recall all 1.0), indicating overfitting.\n",
    "- **Validation Set**: Moderate performance (F1: 0.38, Accuracy: 0.52), with recall for class 0 being high (0.88), but very low recall for class 1 (0.00), suggesting a bias toward class 0.\n",
    "- **Test Set**: Poor performance (F1: 0.28, Accuracy: 0.40), with low precision for class 0 (0.38) and class 1 (0.00), indicating difficulty in generalizing to unseen data.\n",
    "\n",
    "### Comparison (Tuned vs Untuned):\n",
    "- **Test Set**: The tuned model improves significantly in precision (0.60 vs. 0.21), but recall remains low (0.42), indicating better focus on reducing false positives while missing some true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a93aee",
   "metadata": {
    "id": "YMlghpJ22JNf"
   },
   "source": [
    "### Model Selection Based on our Criteria\n",
    "\n",
    "#### Summary Table\n",
    "\n",
    "| Model                      | Dataset       | Accuracy | F1 Score | Precision | Recall |\n",
    "|----------------------------|---------------|----------|----------|-----------|--------|\n",
    "| **Untuned GloVe**          | Training      | 1.00     | 1.00     | 1.00      | 1.00   |\n",
    "|                            | Validation    | 0.46     | 0.25     | -         | -      |\n",
    "|                            | Test          | 0.36     | 0.21     | -         | -      |\n",
    "| **Tuned GloVe**            | Training      | 1.00     | 1.00     | 1.00      | 1.00   |\n",
    "|                            | Validation    | 0.46     | 0.44     | -         | -      |\n",
    "|                            | Test          | 0.36     | 0.33     | -         | -      |\n",
    "| **Untuned Sentence Transformer** | Training | 1.00     | 1.00     | 1.00      | 1.00   |\n",
    "|                            | Validation    | 0.52     | 0.38     | -         | -      |\n",
    "|                            | Test          | 0.40     | 0.28     | -         | -      |\n",
    "| **Tuned Sentence Transformer** | Training  | 1.00     | 1.00     | 1.00      | 1.00   |\n",
    "|                            | Validation    | 0.54     | 0.49     | 0.65      | 0.54   |\n",
    "|                            | Test          | 0.38     | 0.24     | 0.60      | 0.42   |\n",
    "| **Untuned Word2Vec**       | Training      | 1.00     | 1.00     | 1.00      | 1.00   |\n",
    "|                            | Validation    | 0.45     | 0.22     | -         | -      |\n",
    "|                            | Test          | 0.35     | 0.20     | -         | -      |\n",
    "| **Tuned Word2Vec**         | Training      | 1.00     | 1.00     | 1.00      | 1.00   |\n",
    "|                            | Validation    | 0.49     | 0.39     | -         | -      |\n",
    "|                            | Test          | 0.37     | 0.27     | -         | -      |\n",
    "\n",
    "#### Model Selection Analysis\n",
    "\n",
    "1. **F1-Score (Primary Metric)**:\n",
    "   - The **Tuned GloVe** model has the highest F1-score on the test set (0.33), which shows better generalization on unseen data compared to other models. It also has a relatively good F1-score (0.44) on the validation set.\n",
    "   - The **Tuned Sentence Transformer** model has the highest F1-score on the validation set (0.49), showing it may perform slightly better in detecting the minority classes, although its test F1-score is lower (0.24).\n",
    "\n",
    "2. **Accuracy (Complementary Metric)**:\n",
    "   - Both **Tuned GloVe** and **Tuned Sentence Transformer** models perform similarly in terms of accuracy on the test set, with **Tuned GloVe** at 0.36 and **Tuned Sentence Transformer** at 0.38, indicating neither model significantly overfits to the majority class.\n",
    "\n",
    "3. **Model Recommendation**:\n",
    "   - Given the F1-score priority and the slightly better balance of precision and recall, **Tuned GloVe** appears to be the best choice. It performs consistently across validation and test sets, handling the imbalanced dataset better than the alternatives.\n",
    "   - **Tuned Sentence Transformer** is a strong alternative if further tuning is possible, as its high precision on the validation set suggests it could be refined for potentially better minority class detection.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The **Tuned GloVe** model aligns best with your criteria by offering the most consistent F1-scores across validation and test datasets, making it the recommended model for your sentiment analysis system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42c05e",
   "metadata": {
    "id": "zWOVw0CO4Rsx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'model_glove_tuned' is your final trained and tuned GloVe model\n",
    "final_model = model_glove_tuned\n",
    "\n",
    "# Predicting on validation and test sets\n",
    "val_preds = final_model.predict(X_val_glove)\n",
    "test_preds = final_model.predict(X_test_glove)\n",
    "\n",
    "# Creating confusion matrices\n",
    "val_conf_matrix = confusion_matrix(y_val, val_preds)\n",
    "test_conf_matrix = confusion_matrix(y_test, test_preds)\n",
    "\n",
    "# Printing the actual confusion matrix data\n",
    "print(\"Validation Set Confusion Matrix Data:\")\n",
    "print(\"Negative\\tNeutral\\tPositive\")\n",
    "for row in val_conf_matrix:\n",
    "    print(\"\\t\".join(map(str, row)))\n",
    "\n",
    "print(\"\\nTest Set Confusion Matrix Data:\")\n",
    "print(\"Negative\\tNeutral\\tPositive\")\n",
    "for row in test_conf_matrix:\n",
    "    print(\"\\t\".join(map(str, row)))\n",
    "\n",
    "# Plotting confusion matrices for validation and test sets\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Validation set confusion matrix\n",
    "disp_val = ConfusionMatrixDisplay(confusion_matrix=val_conf_matrix, display_labels=['Negative', 'Neutral', 'Positive'])\n",
    "disp_val.plot(ax=ax[0], cmap='Blues', colorbar=False)  # Display without color bar to avoid duplication\n",
    "ax[0].set_title(\"Validation Set Confusion Matrix\")\n",
    "\n",
    "# Test set confusion matrix\n",
    "disp_test = ConfusionMatrixDisplay(confusion_matrix=test_conf_matrix, display_labels=['Negative', 'Neutral', 'Positive'])\n",
    "disp_test.plot(ax=ax[1], cmap='Blues', colorbar=True)  # Display with color bar for consistency\n",
    "ax[1].set_title(\"Test Set Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()  # Adjust layout for readability\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014e6138",
   "metadata": {
    "id": "iIfk96f06QW8"
   },
   "source": [
    "### Observations on the final model:\n",
    "\n",
    "1. **Weakness in Predicting Negative Sentiment**: The model struggles to correctly classify negative sentiment, with very few true negatives in both validation and test sets.\n",
    "\n",
    "2. **Good Performance on Neutral Sentiment**: The model performs better on neutral sentiment, with more correct predictions, though misclassifications still occur.\n",
    "\n",
    "3. **Challenges with Positive Sentiment**: The model has difficulty predicting positive sentiment, often misclassifying it as neutral or negative, especially in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01857a4b",
   "metadata": {
    "id": "0qP5KTLo3OOC"
   },
   "source": [
    "## **Weekly News Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6e520",
   "metadata": {
    "id": "UX5laeu27ZEt"
   },
   "source": [
    "**Important Note**: It is recommended to run this section of the project independently from the previous sections in order to avoid runtime crashes due to RAM overload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cfc98d",
   "metadata": {
    "id": "sSalEobo6mow"
   },
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb97a60",
   "metadata": {
    "id": "Icz7MGhN6DK0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive',force_remount=True)\n",
    "\n",
    "path2='/content/drive/My Drive/AIMLTRAINING/collabdata/NLP/stock_news.csv'\n",
    "df_llm=pd.read_csv(path2)\n",
    "# df_llm=pd.read_csv(\"stock_news.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf740e96",
   "metadata": {
    "id": "nBU3e0bv6wc4"
   },
   "source": [
    "Convert the Date Column to Pandas Date Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095b69a",
   "metadata": {
    "id": "31YQR8-g6zYm"
   },
   "outputs": [],
   "source": [
    "df_llm['Date'] = pd.to_datetime(df_llm['Date'])\n",
    "# Print dataset information and summaries\n",
    "print(\"Dataset Information:\")\n",
    "print(\"-\"*30)\n",
    "print(df_llm.info())  # Provides an overview of the dataframe, including data types and non-null counts\n",
    "\n",
    "print(\"\\nFirst Five Rows of the Dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(df_llm.head())  # Displays the first 5 rows of the dataset\n",
    "\n",
    "print(\"\\nShape of the Dataset:\")\n",
    "print(\"-\"*30)\n",
    "print(f\"Rows: {df_llm.shape[0]}, Columns: {df_llm.shape[1]}\")  # Displays the number of rows and columns\n",
    "\n",
    "print(\"\\nCount of Missing Values in Each Column:\")\n",
    "print(\"-\"*30)\n",
    "print(df_llm.isnull().sum())  # Displays the count of missing values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32170382",
   "metadata": {
    "id": "J6ebpM5XUIK_"
   },
   "source": [
    "**Group the Data by Week**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1795b",
   "metadata": {
    "id": "uqd1L3tL8Otp"
   },
   "outputs": [],
   "source": [
    "# Group the data by week and aggregate\n",
    "df_weekly = df_llm.groupby(pd.Grouper(key='Date', freq='W')).agg({\n",
    "    'News': ' || '.join,  # Combine daily news using '||' as the delimiter\n",
    "    'Open': 'first',      # Use the opening price of the first day of the week\n",
    "    'High': 'max',        # Use the highest price during the week\n",
    "    'Low': 'min',         # Use the lowest price during the week\n",
    "    'Close': 'last',      # Use the closing price of the last day of the week\n",
    "    'Volume': 'sum',      # Sum up the volume traded during the week\n",
    "    'Label': 'last'       # Use the label of the last day of the week\n",
    "}).reset_index()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# Display the resulting DataFrame\n",
    "print(\"Weekly Aggregated Data:\")\n",
    "print(\"-\"*30)\n",
    "print(df_weekly.head())  # Print first 5 rows to preview\n",
    "\n",
    "# Preview the shape of the aggregated data\n",
    "print(\"\\nShape of Aggregated Weekly Data:\")\n",
    "print(\"-\"*30)\n",
    "print(df_weekly.shape)\n",
    "\n",
    "# Check for any missing values in the aggregated data\n",
    "print(\"\\nMissing Values in Aggregated Weekly Data:\")\n",
    "print(\"-\"*30)\n",
    "print(df_weekly.isnull().sum())\n",
    "\n",
    "# Display information about the aggregated data\n",
    "print(\"\\nAggregated Weekly Data Info:\")\n",
    "print(\"-\"*30)\n",
    "print(df_weekly.info())\n",
    "\n",
    "# Display first few rows of the dataset after grouping and aggregation\n",
    "print(\"\\nFirst Five Rows After Grouping and Aggregation:\")\n",
    "print(\"-\"*30)\n",
    "print(df_weekly.head())\n",
    "\n",
    "# Check how the 'News' column looks after concatenation\n",
    "print(\"\\nExample of 'News' Column After Aggregation:\")\n",
    "print(\"-\"*30)\n",
    "print(df_weekly['News'].head().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108d0ab8",
   "metadata": {
    "id": "QcjNvmJ-Rp-b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "try:\n",
    "  # Force remount Google Drive to refresh the connection\n",
    "  drive.mount('/content/drive', force_remount=True)  # Added force_remount=True\n",
    "  print(\"Drive mounted successfully!\")\n",
    "except ValueError as e:\n",
    "  print(f\"Mount failed: {e}\")\n",
    "  print(\"Check your authentication and try again. You might need to authorize access to your Google Drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2037ce",
   "metadata": {
    "id": "ZA2841nMYC-y"
   },
   "outputs": [],
   "source": [
    "# Define the file path for saving the DataFrame as a CSV file\n",
    "file_path = '/content/drive/My Drive/AIMLTRAINING/collabdata/NLP/df_weekly.csv'\n",
    "\n",
    "# Save the entire DataFrame to a CSV file\n",
    "df_weekly.to_csv(file_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"DataFrame saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9640d87",
   "metadata": {
    "id": "oWvf3R3An5K4"
   },
   "source": [
    "#### Installing and Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a290d42b",
   "metadata": {
    "id": "MSBtvu2jG7dn"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/abetlen/llama-cpp-python.git\n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.45 --force-reinstall --no-cache-dir -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770bc894",
   "metadata": {
    "id": "V-mESljUbcwY"
   },
   "source": [
    "#### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539242e",
   "metadata": {
    "id": "AuoEISFH59m0"
   },
   "source": [
    "**Note**:\n",
    "\n",
    "- The model is expected to summarize the news from the week by identifying the top three positive and negative events that are most likely to impact the price of the stock.\n",
    "\n",
    "- As an output, the model is expected to return a JSON containing two keys, one for Positive Events and one for Negative Events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce223d1e",
   "metadata": {
    "id": "ottdZvWzWWY9"
   },
   "source": [
    "For the project, we need to define the prompt to be fed to the LLM to help it understand the task to perform. The following should be the components of the prompt:\n",
    "\n",
    "1. **Role**: Specifies the role the LLM will be taking up to perform the specified task, along with any specific details regarding the role\n",
    "\n",
    "  - **Example**: `You are an expert data analyst specializing in news content analysis.`\n",
    "\n",
    "2. **Task**: Specifies the task to be performed and outlines what needs to be accomplished, clearly defining the objective\n",
    "\n",
    "  - **Example**: `Analyze the provided news headline and return the main topics contained within it.`\n",
    "\n",
    "3. **Instructions**: Provides detailed guidelines on how to perform the task, which includes steps, rules, and criteria to ensure the task is executed correctly\n",
    "\n",
    "  - **Example**:\n",
    "\n",
    "```\n",
    "Instructions:\n",
    "1. Read the news headline carefully.\n",
    "2. Identify the main subjects or entities mentioned in the headline.\n",
    "3. Determine the key events or actions described in the headline.\n",
    "4. Extract relevant keywords that represent the topics.\n",
    "5. List the topics in a concise manner.\n",
    "```\n",
    "\n",
    "4. **Output Format**: Specifies the format in which the final response should be structured, ensuring consistency and clarity in the generated output\n",
    "\n",
    "  - **Example**: `Return the output in JSON format with keys as the topic number and values as the actual topic.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bebb39",
   "metadata": {
    "id": "1C7HTeXVYn8i"
   },
   "source": [
    "**Full Prompt Example**:\n",
    "\n",
    "```\n",
    "You are an expert data analyst specializing in news content analysis.\n",
    "\n",
    "Task: Analyze the provided news headline and return the main topics contained within it.\n",
    "\n",
    "Instructions:\n",
    "1. Read the news headline carefully.\n",
    "2. Identify the main subjects or entities mentioned in the headline.\n",
    "3. Determine the key events or actions described in the headline.\n",
    "4. Extract relevant keywords that represent the topics.\n",
    "5. List the topics in a concise manner.\n",
    "\n",
    "Return the output in JSON format with keys as the topic number and values as the actual topic.\n",
    "```\n",
    "\n",
    "**Sample Output**:\n",
    "\n",
    "`{\"1\": \"Politics\", \"2\": \"Economy\", \"3\": \"Health\" }`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ffac21",
   "metadata": {
    "id": "q5ACqb0C7lxn"
   },
   "source": [
    "#### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e54cd",
   "metadata": {
    "id": "KcRnKYZnCulX"
   },
   "outputs": [],
   "source": [
    "# Function to download the model from the Hugging Face model hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Importing the Llama class from the llama_cpp module\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Importing the library for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm # For progress bar related functionalities\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a514c8ce",
   "metadata": {
    "id": "e5NFGkfIaSMT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(f\"Number of CPU cores: {os.cpu_count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53428719",
   "metadata": {
    "id": "MtnDYTc80Pj3"
   },
   "outputs": [],
   "source": [
    "# Ensure you have the necessary libraries imported\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_cpp import Llama  # Make sure to use the correct Llama class if from llama_cpp\n",
    "\n",
    "# Model configuration\n",
    "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
    "model_basename = \"llama-2-13b-chat.Q5_K_M.gguf\"\n",
    "\n",
    "# Download model files from HuggingFace Hub\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=model_name_or_path,\n",
    "    filename=model_basename\n",
    ")\n",
    "\n",
    "# Initialize the Llama model (if using llama_cpp)\n",
    "lcpp_llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_threads=2,  # Number of CPU threads (adjust as per your system)\n",
    "    n_batch=512,  # Batch size; ensure it fits within your GPU's VRAM\n",
    "    n_gpu_layers=10,  # Number of GPU layers, modify according to GPU VRAM\n",
    "    n_ctx=5096,  # Context window size (modify if necessary)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c1dfb1",
   "metadata": {
    "id": "re3HZr3BcB5v"
   },
   "source": [
    "##### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653f81d",
   "metadata": {
    "id": "3Zy4GD2DcF9z"
   },
   "outputs": [],
   "source": [
    "# defining a function to parse the JSON output from the model\n",
    "def extract_json_data(json_str):\n",
    "    import json\n",
    "    try:\n",
    "        # Find the indices of the opening and closing curly braces\n",
    "        json_start = json_str.find('{')\n",
    "        json_end = json_str.rfind('}')\n",
    "\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            extracted_category = json_str[json_start:json_end + 1]  # Extract the JSON object\n",
    "            data_dict = json.loads(extracted_category)\n",
    "            return data_dict\n",
    "        else:\n",
    "            print(f\"Warning: JSON object not found in response: {json_str}\")\n",
    "            return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a88926",
   "metadata": {
    "id": "XwtB-hG4h9GL"
   },
   "source": [
    "##### Defining the response function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4eda6a",
   "metadata": {
    "id": "wNXpBE7sDrr-"
   },
   "outputs": [],
   "source": [
    "# Function to generate, process, and return the response from the LLaMA model\n",
    "def response_llama(prompt, news, model):\n",
    "\n",
    "    # System message to guide the model's behavior\n",
    "    system_message = \"\"\"\n",
    "    [INST]<<SYS>> Respond to the user question based on the user prompt<</SYS>>[/INST]\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine user prompt with the system message for better context\n",
    "    full_prompt = f\"{prompt}\\nNews Articles: {news}\\n{system_message}\"\n",
    "\n",
    "    # Generate the response with model parameters for better quality\n",
    "    response = model(\n",
    "        prompt=full_prompt,\n",
    "        max_tokens=512,           # Increase max tokens for longer responses\n",
    "        temperature=0.01,          # Low temperature for deterministic output\n",
    "        top_p=0.95,                # Allows more diverse token sampling\n",
    "        repeat_penalty=1.2,        # Penalize repeated tokens\n",
    "        top_k=50,                  # Limits token sampling to top 50\n",
    "        stop=['INST'],             # Stop generating at the end of instruction\n",
    "        echo=False                 # Do not echo the input in the output\n",
    "    )\n",
    "\n",
    "    # Extract and return the response text\n",
    "    response_text = response[\"choices\"][0][\"text\"]\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f996258",
   "metadata": {
    "id": "osPo6gq_0vZa"
   },
   "source": [
    "**Note**: Use this section to test out the prompt with one instance before using it for the entire weekly data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19495548",
   "metadata": {
    "id": "ihu_57lgOPnG"
   },
   "outputs": [],
   "source": [
    "prompt='''Analyze the provided news articles and do the following:\n",
    "\n",
    "1. Identify the top three positive events that are likely to impact the stock price.\n",
    "\n",
    "2. Identify the top three negative events that are likely to impact the stock price.\n",
    "\n",
    "3. Select the top 10 keywords in the News\n",
    "\n",
    "4. PROVIDE YOUR RESPONSE IN JSON FORMAT ONLY\n",
    "\n",
    "5. Do not provide any additional content\n",
    "\n",
    "6. Follow the output example below\n",
    "7. STRICTLY PROVIDE JSON RESPONSE ONLY.\n",
    "\n",
    "Example JSON Output:\n",
    "{\n",
    "\n",
    "  \"top_positive_events\": [\n",
    "\n",
    "    \"Company announces record-breaking quarterly earnings.\",\n",
    "\n",
    "    \"Successful product launch with positive reviews.\",\n",
    "\n",
    "    \"Strategic partnership with a major industry player.\"\n",
    "\n",
    "  ],\n",
    "\n",
    "  \"top_negative_events\": [\n",
    "\n",
    "    \"Cybersecurity breach exposes sensitive customer data.\",\n",
    "\n",
    "    \"CEO resigns amid scandal.\",\n",
    "\n",
    "    \"Economic downturn impacting industry sales.\"\n",
    "\n",
    "  ],\n",
    "\n",
    "  \"top_10_keywords\": [\n",
    "\n",
    "    \"company\", \"product\", \"earnings\", \"cybersecurity\", \"breach\", \"CEO\", \"recession\", \"economy\", \"sales\", \"partnership\"\n",
    "\n",
    "  ]\n",
    "\n",
    "}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94956bd9",
   "metadata": {
    "id": "JRsgiVSXbcwi"
   },
   "source": [
    "##### Checking the model output on a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2bb0d9",
   "metadata": {
    "id": "lEnQHt_nxcu7"
   },
   "outputs": [],
   "source": [
    "#Test response for just one row\n",
    "news=df_weekly['News'][1]\n",
    "# print(news)\n",
    "response=response_llama(prompt,news,lcpp_llm)\n",
    "print(response)\n",
    "df_weekly.at[1, 'Model_Response'] = response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584b2373",
   "metadata": {
    "id": "gcjS5edgBP4A"
   },
   "source": [
    "Run for the entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1a1b0",
   "metadata": {
    "id": "syajCQ25SrEQ"
   },
   "outputs": [],
   "source": [
    "# Process rows in the DataFrame\n",
    "for index, record in df_weekly.iterrows():\n",
    "    news = record['News']  # Extract the 'News' data from the current row\n",
    "    if index==1: continue\n",
    "    try:\n",
    "        # Use the updated response_llama function to get the model's response\n",
    "        response = response_llama(prompt, news, lcpp_llm)  # Pass the correct arguments\n",
    "        df_weekly.at[index, 'Model_Response'] = response\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {e}\")\n",
    "        df_weekly.at[index, 'Model_Response'] = \"Error\"\n",
    "\n",
    "# Display the updated DataFrame with the model's response as a new column\n",
    "print(df_weekly.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd92e10",
   "metadata": {
    "id": "vwv4En8t5TAU"
   },
   "outputs": [],
   "source": [
    "# Step 5: Save the updated DataFrame to Google Drive as a CSV file\n",
    "import os  # Importing the OS module for file handling\n",
    "\n",
    "path = '/content/drive/My Drive/AIMLTRAINING/collabdata/NLP/Response_weekly2.csv'\n",
    "df_weekly.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d1b21",
   "metadata": {
    "id": "W1RV6ojhME7P"
   },
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive',force_remount=True)\n",
    "\n",
    "path = '/content/drive/My Drive/AIMLTRAINING/collabdata/NLP/Response_weekly2.csv'\n",
    "df_weekly_processed=pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74fff00",
   "metadata": {
    "id": "JAIvj_RFhrGd"
   },
   "outputs": [],
   "source": [
    "#drop News\n",
    "df_weekly_processed.drop(columns=['News'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f57552",
   "metadata": {
    "id": "nFNa_MMwTCyA"
   },
   "source": [
    "##### Checking the model output on the weekly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c523cddb",
   "metadata": {
    "id": "hYGUlu2st2Oz"
   },
   "outputs": [],
   "source": [
    "df_weekly_processed.sample(2).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6961e8",
   "metadata": {
    "id": "C-MW3XpmQfqN"
   },
   "source": [
    "##### Formatting the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c55b42",
   "metadata": {
    "id": "46mibskmZhsf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json(text):\n",
    "    # Regular expression pattern to match JSON-like structure\n",
    "    json_pattern = r'\\{.*\\}'\n",
    "\n",
    "    # Search for the JSON part\n",
    "    match = re.search(json_pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        # Extract and return the JSON string\n",
    "        json_string = match.group(0)\n",
    "\n",
    "        # Convert the JSON string to a Python dictionary\n",
    "        try:\n",
    "            return json.loads(json_string)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Error decoding JSON.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No JSON found.\")\n",
    "        return None\n",
    "\n",
    "# Create DataFrame\n",
    "df = df_weekly_processed.copy()\n",
    "\n",
    "# Apply the extract_json function to the 'Model_Response' column and create a new 'Response' column\n",
    "df['Response'] = df['Model_Response'].apply(extract_json)\n",
    "\n",
    "# Drop the 'Model_Response' column\n",
    "df.drop(columns=['Model_Response'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd2233",
   "metadata": {
    "id": "T-l_z7OBiK4x"
   },
   "outputs": [],
   "source": [
    "df.sample(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef2048",
   "metadata": {
    "id": "cFgNoqvdLFPu"
   },
   "outputs": [],
   "source": [
    "# Sample a random entry from 'Response'\n",
    "sample_response = df['Response'].sample(1).values[0]\n",
    "\n",
    "# Pretty print the JSON content\n",
    "pretty_json = json.dumps(sample_response, indent=4)\n",
    "\n",
    "# Display the pretty-printed JSON\n",
    "print(pretty_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71c525c",
   "metadata": {
    "id": "PzXNIPJPbDLm"
   },
   "source": [
    "**Processed News Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db72174",
   "metadata": {
    "id": "E24Z46KWa54I"
   },
   "outputs": [],
   "source": [
    "# Analyse the positive and Negative sentiment\n",
    "def analyze_responses(df):\n",
    "    analysis_results = []\n",
    "\n",
    "    # Initialize counters for sentiment types\n",
    "    positive_sentiment_count = 0\n",
    "    negative_sentiment_count = 0\n",
    "\n",
    "    for response in df['Response']:\n",
    "        # Extract positive and negative events\n",
    "        positive_events = response.get(\"top_positive_events\", [])\n",
    "        negative_events = response.get(\"top_negative_events\", [])\n",
    "\n",
    "        # Count the number of positive and negative events\n",
    "        positive_count = len(positive_events)\n",
    "        negative_count = len(negative_events)\n",
    "\n",
    "        # Append analysis results\n",
    "        analysis_results.append({\n",
    "            \"positive_count\": positive_count,\n",
    "            \"negative_count\": negative_count\n",
    "        })\n",
    "\n",
    "    # Add the analysis results as a new column to the dataframe\n",
    "    df['Analysis'] = analysis_results\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f18f30",
   "metadata": {
    "id": "xILWXs3OilOj"
   },
   "outputs": [],
   "source": [
    "df['Analysis']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9797588",
   "metadata": {
    "id": "2aj14NxWmIsx"
   },
   "source": [
    "**Count Most Common Topic Pairs from Positive and Negative Events**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19438f07",
   "metadata": {
    "id": "BHuw1U6bIkFq"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# Create a list to hold pairs of topics\n",
    "topic_pairs = []\n",
    "\n",
    "# Loop through each entry to generate topic pairs\n",
    "for response in df['Response']:\n",
    "    try:\n",
    "        # Extract the relevant topic fields directly from the dictionary\n",
    "        positive_events = response.get(\"top_positive_events\", [])\n",
    "        negative_events = response.get(\"top_negative_events\", [])\n",
    "\n",
    "        # Combine both positive and negative events into a single list\n",
    "        all_events = positive_events + negative_events\n",
    "\n",
    "        # Get all unique pairs of topics (sorted to avoid duplicates)\n",
    "        topic_pairs.extend(itertools.combinations(sorted(all_events), 2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry: {e}\")\n",
    "\n",
    "# Count the frequency of topic pairs\n",
    "pair_counter = Counter(topic_pairs)\n",
    "\n",
    "# Display the most common topic pairs\n",
    "top_pairs = pair_counter.most_common(10)\n",
    "\n",
    "# Print the top 10 most common topic pairs\n",
    "print(top_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bd814",
   "metadata": {
    "id": "uZZSBRQ7i57A"
   },
   "source": [
    "**Insights**\n",
    "\n",
    "- **Sentiment Distribution**: Despite the varied events, there has been no occurrence of strong positive or negative sentiment. This could indicate that the events analyzed are seen as balanced or neutral, meaning there isn't a clear sentiment trend, possibly reflecting mixed market perceptions in the data during the given timeframe.\n",
    "- **Apple's Revenue and Economic Concerns:** Apple's revenue warning is frequently paired with concerns about the global economy and stock market declines.\n",
    "\n",
    "- **Trade Optimism vs Market Reactions:** Trade optimism is often linked to contrasting market movements, such as stock declines and oil price rebounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1b3dc8",
   "metadata": {
    "id": "9qCj1kK2HTei"
   },
   "source": [
    "**Keyword Analysis**\n",
    "- Occurences in the weekly News data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652da17d",
   "metadata": {
    "id": "pSYYfSCb0pYw"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize a Counter to track keyword frequencies\n",
    "keyword_counter = Counter()\n",
    "\n",
    "# Loop through each entry in the column\n",
    "for response in df['Response']:\n",
    "    try:\n",
    "        # Directly access the 'top_10_keywords' list from the dictionary\n",
    "        keywords = response.get(\"top_10_keywords\", [])\n",
    "\n",
    "        # Update the counter with the keywords in this entry\n",
    "        keyword_counter.update(keywords)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry: {e}\")\n",
    "\n",
    "# Show the most common keywords\n",
    "print(keyword_counter.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ab86ed",
   "metadata": {
    "id": "gfgZ2kT-KQD3"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample a random entry from 'Response' column\n",
    "sample_response = df['Response'].sample(1).values[0]\n",
    "\n",
    "# Access the 'top_10_keywords' list directly from the dictionary\n",
    "keywords = sample_response.get(\"top_10_keywords\", [])\n",
    "\n",
    "# Count the frequency of each keyword (using 1 as a dummy count for this sample)\n",
    "keyword_counter = {keyword: 1 for keyword in keywords}\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(keyword_counter)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off the axis\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fbfd35",
   "metadata": {
    "id": "33LyaxF6nAcX"
   },
   "source": [
    "**Keyword Pair Frequency Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7870e",
   "metadata": {
    "id": "b2Pe4Uk6l8iQ"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# Create a list to hold pairs of keywords\n",
    "keyword_pairs = []\n",
    "\n",
    "# Loop through each entry to generate keyword pairs\n",
    "for response in df['Response']:\n",
    "    try:\n",
    "        # Extract the top 10 keywords directly from the dictionary\n",
    "        keywords = response.get(\"top_10_keywords\", [])\n",
    "\n",
    "        # Get all unique pairs of keywords (sorted to avoid duplicates)\n",
    "        keyword_pairs.extend(itertools.combinations(sorted(keywords), 2))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry: {e}\")\n",
    "\n",
    "# Count the frequency of keyword pairs\n",
    "pair_counter = Counter(keyword_pairs)\n",
    "\n",
    "# Display the most common keyword pairs\n",
    "top_keyword_pairs = pair_counter.most_common(10)\n",
    "\n",
    "# Print the top 10 most common keyword pairs\n",
    "print(top_keyword_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f681c",
   "metadata": {
    "id": "ERU95zqtHlRj"
   },
   "source": [
    "**Insight on Keyword occurences:**\n",
    "- **Dominance of Apple and China:** The frequent mention of \"Apple\" and \"China\" highlights their central role in the topics, indicating their significance in trade and economic discussions.\n",
    "\n",
    "- **Tech and Trade Themes:** Keywords like \"Trade,\" \"Tech,\" and \"Stocks\" suggest a strong focus on technology and international trade, with an emphasis on market movements.\n",
    "- **Apple and China are frequently mentioned together**, with the most common pair appearing 11 times, highlighting their strong connection in the dataset.\n",
    "\n",
    "- **Tech-related terms like \"Apple\", \"China\", and \"Trade\" are often paired together**, indicating a significant focus on the intersection of technology, international trade, and market dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e7f967",
   "metadata": {
    "id": "HiOLoD7BO3L-"
   },
   "source": [
    "## **Conclusions and Recommendations**\n",
    "\n",
    "### Conclusion: Stock Market Sentiment Analysis\n",
    "\n",
    "#### Summary\n",
    "This project aimed to analyze the relationship between news articles and stock market sentiment, leveraging natural language processing (NLP) and machine learning techniques to generate actionable insights.\n",
    "\n",
    "#### Key Findings\n",
    "- **Dominant Themes:** Business, Finance, Technology, China, and Economy were the most prevalent themes in the news articles, playing a significant role in influencing stock prices and market sentiment.\n",
    "  \n",
    "- **Sentiment Analysis:** News sentiment strongly impacts stock prices, with negative sentiment causing larger price movements compared to positive or neutral sentiment.\n",
    "\n",
    "- **Topic Modeling:** Common topic pairs such as **Business-Finance**, **Business-Technology**, and **China-Economy** exhibit strong correlations, highlighting areas that influence investor sentiment and market dynamics. Apple and China were particularly central, with **\"Apple\"** and **\"China\"** frequently mentioned together, pointing to their significance in trade and economic discussions.\n",
    "\n",
    "- **Model Performance:** The **Tuned GloVe** model outperformed other models in terms of F1-score on the test set (0.33), showcasing its ability to generalize well despite the imbalanced dataset. It consistently performed well in both validation and test sets, making it the most reliable for stock market sentiment analysis.\n",
    "\n",
    "#### Recommendations\n",
    "\n",
    "1. **Integrate NLP into Stock Market Analysis:**\n",
    "   - Leverage sentiment analysis and topic modeling to enhance stock market prediction systems. By evaluating news sentiment, you can better understand market trends and improve forecasting accuracy.\n",
    "\n",
    "2. **Monitor Key Topic Pairs:**\n",
    "   - Focus on monitoring **Business-Finance**, **Business-Technology**, and **China-Economy** as these topics strongly correlate with stock price fluctuations and investor sentiment. Tracking sentiment around these topics can provide deeper insights into market movements.\n",
    "\n",
    "3. **Implement Sentiment Analysis for Investment Decisions:**\n",
    "   - Use sentiment analysis to aid in short-term investment strategies. For instance, **positive** or **negative** sentiment in news articles can serve as early indicators for potential market changes. Investors could incorporate sentiment signals to adjust their portfolios accordingly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb2fb7b",
   "metadata": {
    "id": "5r0z8qUZ4MOE"
   },
   "source": [
    "-\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
